{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O-09FurB50CN",
        "outputId": "5c79104a-9ee5-4b2c-b0fa-4815c1c3e55d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TSV file found: spoc-train.tsv\n",
            "                                        pseudocode  \\\n",
            "0                in the function gcd(a,b=integers)   \n",
            "1  if b=1 return a, else call function gcd(b, a%b)   \n",
            "2                                              NaN   \n",
            "3                                              NaN   \n",
            "4               n , nn, ans = integers with ans =0   \n",
            "\n",
            "                             code  \n",
            "0         int gcd(int a, int b) {  \n",
            "1  return !b ? a : gcd(b, a % b);  \n",
            "2                               }  \n",
            "3                    int main() {  \n",
            "4             int n, nn, ans = 0;  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define file path\n",
        "uploaded_tsv_path = \"spoc-train.tsv\"\n",
        "\n",
        "# Check if file exists\n",
        "if os.path.exists(uploaded_tsv_path):\n",
        "    print(\"TSV file found:\", uploaded_tsv_path)\n",
        "else:\n",
        "    raise FileNotFoundError(\"TSV file not found!\")\n",
        "\n",
        "# Load TSV file\n",
        "df = pd.read_csv(uploaded_tsv_path, sep=\"\\t\")\n",
        "\n",
        "# Rename columns\n",
        "df.rename(columns={\"text\": \"pseudocode\", \"code\": \"code\"}, inplace=True)\n",
        "\n",
        "# Define CSV file path\n",
        "csv_path = \"spoc-train.csv\"\n",
        "\n",
        "# Save selected columns to CSV\n",
        "df[[\"pseudocode\", \"code\"]].to_csv(csv_path, index=False)\n",
        "\n",
        "# Load CSV file for further operations\n",
        "df_csv = pd.read_csv(csv_path)\n",
        "\n",
        "# Print first five rows of the CSV-loaded DataFrame\n",
        "print(df_csv.head(5))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(text):\n",
        "    return \"\" if pd.isna(text) else str(text)\n",
        "\n",
        "df_csv[\"pseudocode\"] = df_csv[\"pseudocode\"].apply(clean_text)\n",
        "df_csv[\"code\"] = df_csv[\"code\"].apply(clean_text)\n",
        "print(df_csv.head(5))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zxaS1oMi6rvx",
        "outputId": "cda41975-7c9d-40d5-e5cf-55d14232eb83"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                        pseudocode  \\\n",
            "0                in the function gcd(a,b=integers)   \n",
            "1  if b=1 return a, else call function gcd(b, a%b)   \n",
            "2                                                    \n",
            "3                                                    \n",
            "4               n , nn, ans = integers with ans =0   \n",
            "\n",
            "                             code  \n",
            "0         int gcd(int a, int b) {  \n",
            "1  return !b ? a : gcd(b, a % b);  \n",
            "2                               }  \n",
            "3                    int main() {  \n",
            "4             int n, nn, ans = 0;  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Assuming df_csv is already loaded and cleaned\n",
        "# e.g., df_csv = pd.read_csv(\"/kaggle/working/spoc-train.csv\")\n",
        "# df_csv[\"pseudocode\"] = df_csv[\"pseudocode\"].apply(lambda x: \"\" if pd.isna(x) else str(x))\n",
        "# df_csv[\"code\"] = df_csv[\"code\"].apply(lambda x: \"\" if pd.isna(x) else str(x))\n",
        "\n",
        "def reassign_code(df_csv):\n",
        "    # Create a working copy of the DataFrame\n",
        "    df_csv = df_csv.copy()\n",
        "\n",
        "    # Iterate through the DataFrame rows\n",
        "    i = 0\n",
        "    while i < len(df_csv):\n",
        "        # Check if current row has missing pseudocode\n",
        "        if df_csv.loc[i, \"pseudocode\"] == \"\":\n",
        "            # Skip if it's the first row (no previous row to assign to)\n",
        "            if i == 0:\n",
        "                i += 1\n",
        "                continue\n",
        "\n",
        "            # Check for consecutive missing pseudocode\n",
        "            if i + 1 < len(df_csv) and df_csv.loc[i + 1, \"pseudocode\"] == \"\":\n",
        "                # Handle consecutive case\n",
        "                # 1. Move current row's code to previous row (append)\n",
        "                if df_csv.loc[i, \"code\"] != \"\":\n",
        "                    if df_csv.loc[i - 1, \"code\"] != \"\":\n",
        "                        df_csv.loc[i - 1, \"code\"] += \"\\n\" + df_csv.loc[i, \"code\"]\n",
        "                    else:\n",
        "                        df_csv.loc[i - 1, \"code\"] = df_csv.loc[i, \"code\"]\n",
        "                    df_csv.loc[i, \"code\"] = \"\"  # Clear moved code\n",
        "\n",
        "                # 2. Move next row's code to the next valid pseudocode row (prepend)\n",
        "                if i + 1 < len(df_csv) and df_csv.loc[i + 1, \"code\"] != \"\":\n",
        "                    for j in range(i + 1, len(df_csv)):\n",
        "                        if df_csv.loc[j, \"pseudocode\"] != \"\":\n",
        "                            if df_csv.loc[j, \"code\"] != \"\":\n",
        "                                # Prepend the second code to the existing code\n",
        "                                df_csv.loc[j, \"code\"] = df_csv.loc[i + 1, \"code\"] + \"\\n\" + df_csv.loc[j, \"code\"]\n",
        "                            else:\n",
        "                                df_csv.loc[j, \"code\"] = df_csv.loc[i + 1, \"code\"]\n",
        "                            df_csv.loc[i + 1, \"code\"] = \"\"  # Clear moved code\n",
        "                            break\n",
        "                    i += 2  # Skip the next row since we processed it\n",
        "                else:\n",
        "                    i += 1\n",
        "            else:\n",
        "                # Single missing pseudocode: move code to previous row (append)\n",
        "                if df_csv.loc[i, \"code\"] != \"\":\n",
        "                    if df_csv.loc[i - 1, \"code\"] != \"\":\n",
        "                        df_csv.loc[i - 1, \"code\"] += \"\\n\" + df_csv.loc[i, \"code\"]\n",
        "                    else:\n",
        "                        df_csv.loc[i - 1, \"code\"] = df_csv.loc[i, \"code\"]\n",
        "                    df_csv.loc[i, \"code\"] = \"\"  # Clear moved code\n",
        "                i += 1\n",
        "        else:\n",
        "            i += 1\n",
        "\n",
        "    return df_csv\n",
        "\n",
        "# Apply the function to df_csv\n",
        "df_csv = reassign_code(df_csv)\n",
        "\n",
        "# Display the result\n",
        "print(df_csv.head(10))  # Adjust to see more rows if needed"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "811NDXfo6sND",
        "outputId": "e291f5fd-b17d-406e-9c51-9735a5d63b56"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                          pseudocode  \\\n",
            "0                  in the function gcd(a,b=integers)   \n",
            "1    if b=1 return a, else call function gcd(b, a%b)   \n",
            "2                                                      \n",
            "3                                                      \n",
            "4                 n , nn, ans = integers with ans =0   \n",
            "5                                             Read n   \n",
            "6                             for i=2 to n-1 execute   \n",
            "7                                        set nn to n   \n",
            "8  while nn is not equal to 0, set ans to ans + n...   \n",
            "9                                                      \n",
            "\n",
            "                                    code  \n",
            "0                int gcd(int a, int b) {  \n",
            "1      return !b ? a : gcd(b, a % b);\\n}  \n",
            "2                                         \n",
            "3                                         \n",
            "4      int main() {\\nint n, nn, ans = 0;  \n",
            "5                              cin >> n;  \n",
            "6     for (int i = 2; i <= n - 1; ++i) {  \n",
            "7                                nn = n;  \n",
            "8  while (nn) ans += nn % i, nn /= i;\\n}  \n",
            "9                                         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "from torch.utils.data import DataLoader\n",
        "import pandas as pd\n",
        "import re\n",
        "from collections import Counter\n",
        "import math\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "\n",
        "# ---\n",
        "# ### 1. Custom Tokenizer\n",
        "# ---\n",
        "class CustomTokenizer:\n",
        "    def __init__(self):\n",
        "        self.word2idx = {}\n",
        "        self.idx2word = {}\n",
        "        self.vocab_size = 0\n",
        "        self.special_tokens = [\"<sos>\", \"<eos>\", \"<pad>\"]\n",
        "\n",
        "    def build_vocab(self, texts):\n",
        "        \"\"\"Build vocabulary from a list of texts.\"\"\"\n",
        "        all_words = Counter()\n",
        "        for text in texts:\n",
        "            # Ensure text is a valid string\n",
        "            if isinstance(text, str):\n",
        "                words = self.tokenize(text)\n",
        "                all_words.update(words)\n",
        "\n",
        "        # Add special tokens first\n",
        "        for token in self.special_tokens:\n",
        "            self.word2idx[token] = self.vocab_size\n",
        "            self.idx2word[self.vocab_size] = token\n",
        "            self.vocab_size += 1\n",
        "\n",
        "        # Add other words\n",
        "        for word, _ in all_words.most_common():\n",
        "            if word not in self.word2idx:\n",
        "                self.word2idx[word] = self.vocab_size\n",
        "                self.idx2word[self.vocab_size] = word\n",
        "                self.vocab_size += 1\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        \"\"\"Tokenize text into words and punctuation.\"\"\"\n",
        "        return re.findall(r'\\w+|[^\\w\\s]', text.lower())\n",
        "\n",
        "    def encode(self, text):\n",
        "        \"\"\"Convert text to token IDs.\"\"\"\n",
        "        tokens = self.tokenize(text)\n",
        "        return [self.word2idx.get(token, self.word2idx[\"<pad>\"]) for token in tokens]\n",
        "\n",
        "    def decode(self, token_ids):\n",
        "        \"\"\"Convert token IDs back to text.\"\"\"\n",
        "        tokens = [self.idx2word.get(idx, \"<unk>\") for idx in token_ids]\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "# Load dataset\n",
        "df_csv = pd.read_csv(\"spoc-train.csv\")  # Replace with your dataset file path\n",
        "\n",
        "# Drop rows with missing values in 'code' or 'pseudocode'\n",
        "df_csv = df_csv.dropna(subset=[\"code\", \"pseudocode\"])\n",
        "\n",
        "# Convert all entries to strings (to handle any non-string values)\n",
        "df_csv[\"code\"] = df_csv[\"code\"].astype(str)\n",
        "df_csv[\"pseudocode\"] = df_csv[\"pseudocode\"].astype(str)\n",
        "\n",
        "# Build vocabulary from the dataset\n",
        "all_texts = df_csv[\"code\"].tolist() + df_csv[\"pseudocode\"].tolist()\n",
        "tokenizer = CustomTokenizer()\n",
        "tokenizer.build_vocab(all_texts)\n",
        "\n",
        "# Special token IDs\n",
        "SOS_TOKEN_ID = tokenizer.word2idx[\"<sos>\"]\n",
        "EOS_TOKEN_ID = tokenizer.word2idx[\"<eos>\"]\n",
        "PAD_TOKEN_ID = tokenizer.word2idx[\"<pad>\"]\n",
        "\n",
        "# Preprocess data with <sos> and <eos>\n",
        "def preprocess_data(row):\n",
        "    code = row[\"code\"]  # Input is now C++ code\n",
        "    pseudo = row[\"pseudocode\"]  # Output is now pseudocode\n",
        "    source = [SOS_TOKEN_ID] + tokenizer.encode(code) + [EOS_TOKEN_ID] if code else [SOS_TOKEN_ID, EOS_TOKEN_ID]\n",
        "    target = [SOS_TOKEN_ID] + tokenizer.encode(pseudo) + [EOS_TOKEN_ID] if pseudo else [SOS_TOKEN_ID, EOS_TOKEN_ID]\n",
        "    return {\"source\": source, \"target\": target}\n",
        "\n",
        "train_data = df_csv.apply(preprocess_data, axis=1).tolist()\n",
        "\n",
        "# ---\n",
        "# ### 2. Dataset and DataLoader\n",
        "# ---\n",
        "class TranslationDataset(data.Dataset):\n",
        "    def __init__(self, data):\n",
        "        self.data = data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return torch.tensor(self.data[idx][\"source\"]), torch.tensor(self.data[idx][\"target\"])\n",
        "\n",
        "def collate_fn(batch):\n",
        "    \"\"\"Pad sequences in a batch to the longest length.\"\"\"\n",
        "    sources, targets = zip(*batch)\n",
        "    sources_padded = pad_sequence(sources, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    targets_padded = pad_sequence(targets, batch_first=True, padding_value=PAD_TOKEN_ID)\n",
        "    return sources_padded, targets_padded\n",
        "\n",
        "dataset = TranslationDataset(train_data)\n",
        "dataloader = DataLoader(dataset, batch_size=16, shuffle=True, collate_fn=collate_fn)\n",
        "\n",
        "# ---\n",
        "# ### 3. Transformer Model\n",
        "# ---\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, d_model, max_len=5000):\n",
        "        super().__init__()\n",
        "        pe = torch.zeros(max_len, d_model)\n",
        "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
        "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * -(math.log(10000.0) / d_model))\n",
        "        pe[:, 0::2] = torch.sin(position * div_term)\n",
        "        pe[:, 1::2] = torch.cos(position * div_term)\n",
        "        self.pe = pe.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.pe[:, :x.size(1), :].to(x.device)\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "    def __init__(self, num_layers, d_model, num_heads, dff, vocab_size):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
        "        self.pos_encoding = PositionalEncoding(d_model)\n",
        "        self.transformer = nn.Transformer(\n",
        "            d_model=d_model,\n",
        "            nhead=num_heads,\n",
        "            num_encoder_layers=num_layers,\n",
        "            num_decoder_layers=num_layers,\n",
        "            dim_feedforward=dff,\n",
        "            batch_first=True\n",
        "        )\n",
        "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
        "\n",
        "    def forward(self, src, tgt):\n",
        "        src_emb = self.pos_encoding(self.embedding(src))\n",
        "        tgt_emb = self.pos_encoding(self.embedding(tgt))\n",
        "        src_padding_mask = (src == PAD_TOKEN_ID)\n",
        "        tgt_padding_mask = (tgt == PAD_TOKEN_ID)\n",
        "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(tgt.size(1)).to(tgt.device)\n",
        "        out = self.transformer(\n",
        "            src_emb, tgt_emb,\n",
        "            src_key_padding_mask=src_padding_mask,\n",
        "            tgt_key_padding_mask=tgt_padding_mask,\n",
        "            tgt_mask=tgt_mask\n",
        "        )\n",
        "        return self.fc_out(out)\n",
        "\n",
        "# ---\n",
        "# ### 4. Training Setup\n",
        "# ---\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = Transformer(\n",
        "    num_layers=4,\n",
        "    d_model=256,\n",
        "    num_heads=4,\n",
        "    dff=1024,\n",
        "    vocab_size=tokenizer.vocab_size\n",
        ").to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_TOKEN_ID)\n",
        "optimizer = optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
        "\n",
        "def train_epoch(model, dataloader, optimizer, criterion):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    for src, tgt in dataloader:\n",
        "        src, tgt = src.to(device), tgt.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        output = model(src, tgt[:, :-1])\n",
        "        loss = criterion(output.reshape(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(dataloader)\n",
        "\n",
        "# Train for 15 epochs\n",
        "for epoch in range(15):\n",
        "    loss = train_epoch(model, dataloader, optimizer, criterion)\n",
        "    print(f\"Epoch {epoch+1}/15, Loss: {loss:.4f}\")\n",
        "\n",
        "# Save the model\n",
        "torch.save(model.state_dict(), \"transformer_model.pth\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yKNQrJr66wLe",
        "outputId": "406883b2-9714-4b97-a48e-3390e3f7753b"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15, Loss: 1.3945\n",
            "Epoch 2/15, Loss: 0.8836\n",
            "Epoch 3/15, Loss: 0.7750\n",
            "Epoch 4/15, Loss: 0.7337\n",
            "Epoch 5/15, Loss: 0.7134\n",
            "Epoch 6/15, Loss: 0.6989\n",
            "Epoch 7/15, Loss: 0.6881\n",
            "Epoch 8/15, Loss: 0.6832\n",
            "Epoch 9/15, Loss: 0.6797\n",
            "Epoch 10/15, Loss: 0.6774\n",
            "Epoch 11/15, Loss: 0.6776\n",
            "Epoch 12/15, Loss: 0.6752\n",
            "Epoch 13/15, Loss: 0.6756\n",
            "Epoch 14/15, Loss: 0.6754\n",
            "Epoch 15/15, Loss: 0.6738\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_pseudocode(model, cpp_code, max_len=100):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        # Split C++ code into individual lines\n",
        "        cpp_lines = cpp_code.strip().split('\\n')\n",
        "        generated_pseudo_lines = []\n",
        "\n",
        "        for line in cpp_lines:\n",
        "            # Tokenize the current line of C++ code\n",
        "            src_tokens = [SOS_TOKEN_ID] + tokenizer.encode(line) + [EOS_TOKEN_ID]\n",
        "            src = torch.tensor([src_tokens]).to(device)\n",
        "            tgt = torch.tensor([[SOS_TOKEN_ID]]).to(device)\n",
        "\n",
        "            # Generate pseudocode for the current line\n",
        "            for _ in range(max_len):\n",
        "                output = model(src, tgt)\n",
        "                next_token = output[:, -1, :].argmax(dim=-1).item()\n",
        "                if next_token == EOS_TOKEN_ID:\n",
        "                    break\n",
        "                tgt = torch.cat([tgt, torch.tensor([[next_token]]).to(device)], dim=1)\n",
        "\n",
        "            # Decode the generated tokens and add to results\n",
        "            generated_pseudo_lines.append(tokenizer.decode(tgt[0].tolist()))\n",
        "\n",
        "        # Join all generated lines into a single string\n",
        "        return \"\\n\".join(generated_pseudo_lines)\n",
        "\n",
        "# Load and test the model\n",
        "model.load_state_dict(torch.load(\"transformer_model.pth\", map_location=device))\n",
        "\n",
        "# Example multi-line C++ code input\n",
        "test_cpp = \"\"\"\n",
        "int main() {\n",
        "    string name;\n",
        "    cout << \"Enter your name: \";\n",
        "    cin >> name;\n",
        "    cout << \"Hello, \" << name << \"! Welcome to C++ programming.\" << std::endl;\n",
        "\n",
        "    return 0;\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "print(f\"C++ Code:\\n{test_cpp}\")\n",
        "print(f\"Generated Pseudocode:\\n{generate_pseudocode(model, test_cpp)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOdveQ2t7Bfi",
        "outputId": "ddc38446-7024-41bf-a28f-e6a033dfdf26"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-34-5bf413152b8c>:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict(torch.load(\"transformer_model.pth\", map_location=device))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C++ Code:\n",
            "\n",
            "int main() {\n",
            "    string name;\n",
            "    cout << \"Enter your name: \";\n",
            "    cin >> name;\n",
            "    cout << \"Hello, \" << name << \"! Welcome to C++ programming.\" << std::endl;\n",
            "\n",
            "    return 0;\n",
            "}\n",
            "\n",
            "Generated Pseudocode:\n",
            "<sos> in the function maxn that takes integer ) and returns integer\n",
            "<sos> name = string\n",
            "<sos> print \" | \"\n",
            "<sos> read name\n",
            "<sos> print \" for \" , name , \" \" , c + to the standard output : \" , and print newline\n",
            "<sos> ;\n",
            "<sos> return 0\n",
            "<sos> end statement\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Convert tokenizer object to a dictionary (assuming it has a vocabulary attribute)\n",
        "tokenizer_dict = tokenizer.__dict__  # Extracts attributes as a dictionary\n",
        "\n",
        "# Save as JSON\n",
        "with open(\"tokenizer.json\", \"w\") as f:\n",
        "    json.dump(tokenizer_dict, f)\n"
      ],
      "metadata": {
        "id": "zD2BoQU-NakB"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GD3mNtmEqseJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}